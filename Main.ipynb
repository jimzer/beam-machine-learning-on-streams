{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0o_pH41bp6z"
      },
      "source": [
        "# Machine Learning on Streams with Apache Beam\n",
        "\n",
        "Read-only link: [notebook](https://colab.research.google.com/drive/1RWtxEWsjzlrltbx4zrKMYOUt_P0KCkwA?usp=sharing)\n",
        "\n",
        "\n",
        "# What we will build \n",
        "\n",
        "In this workshop, we will learn how to use Apache Beam to detect potential disasters occurring in the world. We will receive a stream of Tweets as input, use machine learning to detect anomalies, and send alert events in an output stream. These alerts can be consumed in real-time by any service of interest.\n",
        "\n",
        "\n",
        "# Streaming Data Processing\n",
        "\n",
        "Data processing is either offline or online. There are a lot of applications    from streaming/online machine learning, where you receive a continuous flux of data to transform. The output can then be either stored or directly sent to another output stream.\n",
        "For instance, detecting anomalies in a continuous stream of events (IoT sensors, transactions, ...) and sending alterts to an output stream.\n",
        "\n",
        "\n",
        "# Apache Beam\n",
        "\n",
        "**Apache Beam** is a unified programming model to define batch and **streaming** data processing jobs. It's compatible with multiple executor engine such as *Apache Spark*, *Apache Flink* or *Google DataFlow*. It follows the philosopy \"Write once, run everywhere\" and has SDK in multiple languages, i.e **Python**, Java and Go.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WujCPEIqJnaL"
      },
      "source": [
        "\n",
        "## Asumptions\n",
        "\n",
        "We assume a basic knowledge in machine learning (mainly Pandas and SKlearn) and Python programming language.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQIgef4OJ4q9"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "The dataset comes from a Kaggle competition: [Natural Language Processing with Disaster Tweets\n",
        "](https://www.kaggle.com/c/nlp-getting-started).\n",
        "\n",
        "The objective is to predict which Tweets talks about about natural disasters and which does not.\n",
        "\n",
        "Example of disaster Tweet:\n",
        "> Heard about #earthquake in different cities, stay safe everyone\n",
        "\n",
        "Example of normal Tweet:\n",
        "> No don't tell me that!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m25DTXxUJ_A4"
      },
      "source": [
        "# Technologies\n",
        "1. **Google Cloud Storage**: bucket to store the data and the trained model\n",
        "2. **Apache Beam**: Python SDK to create data processing job\n",
        "3. **Google Cloud Pub/Sub**: ingestion platform for event-driven systems and streaming analytics\n",
        "4. **Google Cloud DataFlow**: cloud executor for job expressed with Apache Beam SDK\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TJBGkOrKE3a"
      },
      "source": [
        "\n",
        "# Overall System\n",
        "![overall pipeline](https://i.postimg.cc/sDSB2Tzm/overall-pipeline.png\n",
        "\"Overall System\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07PIO-DaKHrk"
      },
      "source": [
        "\n",
        "# Beam Pipeline\n",
        "\n",
        "![beam pipeline](https://i.postimg.cc/T1rRkTD9/beam-pipeline.png\n",
        "\"Beam Pipeline\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytaY3jMyKQPj"
      },
      "source": [
        "# Plan\n",
        "\n",
        "1. Setup libraries and configuration\n",
        "2. Prepare the input (Tweets) and output (predictions) streams\n",
        "3. Train a simple model and save it in a Google Cloud bucket\n",
        "4. Build a Beam streaming pipeline to process the Tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5sXeRTiggvd"
      },
      "source": [
        "# Setup and Configuration\n",
        "Import libraries and authenticate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN5JE5va8pk9"
      },
      "source": [
        "# Do not forget to restart the runtime after running this cell\n",
        "# or the libraries won't be available\n",
        "!pip install --quiet google-cloud-pubsub\n",
        "!pip install --quiet apache-beam[gcp]\n",
        "!pip install --quiet fsspec gcsfs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWOZQnsF88-6"
      },
      "source": [
        "from typing import Tuple, List, Dict, Iterable, Any\n",
        "\n",
        "import google.auth\n",
        "from google.colab import auth\n",
        "from google.cloud import pubsub as pubsub\n",
        "from google.api_core import exceptions as gexc"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYvnyBiCbcDM"
      },
      "source": [
        "# { display-mode: \"form\" }\n",
        "project_id = \"starthack-workshop\" #@param {type:\"string\"}"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcMgDKRGNKhN",
        "outputId": "4d62bdb8-063e-4ea8-cf85-e93e9bb843a2"
      },
      "source": [
        "%env GCLOUD_PROJECT=starthack-workshop"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: GCLOUD_PROJECT=starthack-workshop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp8GyRqvbsTs"
      },
      "source": [
        "auth.authenticate_user()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znGc0CGh1YGl"
      },
      "source": [
        "# A glance at Google Cloud Pub/Sub\n",
        "\n",
        "It's the service used in this workshop to handle stream of events. We briefly introduce the 2 main concepts to understand how it works: **Topics** and **Subscriptions**.\n",
        "\n",
        "\n",
        "## Topics: enqueuing messages\n",
        "\n",
        "A topic can be seen as queue. It's the place where you push/enqueue messages. In our setup we want a topic to push Tweets (input) and a topic to push predictions (output).\n",
        "\n",
        "## Subscriptions: dequeuing (consuming) messages\n",
        "\n",
        "Now that messages are pushed to the topics (*queues*) we use subscriptions to pull messages out of it (*dequeue*). Any application who need to access message in the queue does it through a subscription. In our case we want a subscription to pull Tweets for the first queue and a subscription to pull predictions from the second queue.\n",
        "\n",
        "A topic can have multiple subscriptions. It will deliver the messages once to every subscriptions. \n",
        "\n",
        "\n",
        "\n",
        "## Overview\n",
        "![overall pipeline](https://i.postimg.cc/zvL1f2Rk/pub-sub.png\n",
        "\"Overall System\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM-plqxfb35x"
      },
      "source": [
        "# Create the publisher and subscriber clients\n",
        "sub_client = pubsub.SubscriberClient()\n",
        "pub_client = pubsub.PublisherClient()\n",
        "# Choose a topic name for input and output steams\n",
        "topic_path_in = pub_client.topic_path(project_id, \"tweets-in\")\n",
        "topic_path_out = pub_client.topic_path(project_id, \"predictions-out\")\n",
        "# Choose subscriptions name\n",
        "subscription_path_in = sub_client.subscription_path(project_id, \"subscription_in\")\n",
        "subscription_path_out = sub_client.subscription_path(project_id, \"subscription_out\")\n",
        "# Sotre the project id in a variable\n",
        "project_path=f\"projects/{project_id}\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3CjLGDxg5cb"
      },
      "source": [
        "# Topic creation\n",
        "\n",
        "First we need to create Pub/Sub topics to push and pull stream of events. In our case we wil have one topic for incoming Tweets *tweets-in* and one topic for outgoing predictions *predictions-out*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL2UHUiwcY41",
        "outputId": "818b9cea-1e4d-4c8f-b885-b234f0f154ab"
      },
      "source": [
        "try:\n",
        "  topic_in = pub_client.create_topic(topic_path_in)\n",
        "  print(f\"Created topic: {topic_in.name}\")\n",
        "except gexc.AlreadyExists:\n",
        "    print(\"Skipping topic creation, already exists\")\n",
        "\n",
        "try:\n",
        "  topic_out = pub_client.create_topic(topic_path_out)\n",
        "  print(f\"Created topic: {topic_out.name}\")\n",
        "except gexc.AlreadyExists:\n",
        "    print(\"Skipping topic creation, already exists\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping topic creation, already exists\n",
            "Skipping topic creation, already exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_XfU4e2c8Xt",
        "outputId": "9da7af26-d005-4779-c8bd-8c70c6887699"
      },
      "source": [
        "for topic in pub_client.list_topics(project_path):\n",
        "    print(topic.name)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "projects/starthack-workshop/topics/tweets-in\n",
            "projects/starthack-workshop/topics/predictions-out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImAIV0ORhBjZ"
      },
      "source": [
        "# Subscription Creation\n",
        "\n",
        "Then we need subscriptions to pull the events pushed to the topics. In our case we have a subscription to get Tweets from the *tweets-in* topic and a subscription to pull predictions from the *predictions-out*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtdqNQJcfxnZ",
        "outputId": "7fdd9e81-d095-414e-cbe3-348e21528448"
      },
      "source": [
        "try:\n",
        "  subscription_in = sub_client.create_subscription(subscription_path_in, topic_path_in)\n",
        "  print(f\"created topic: {subscription_in.name}\")\n",
        "except gexc.AlreadyExists:\n",
        "    print(\"Skipping subscription creation, already exists\")\n",
        "\n",
        "try:\n",
        "  subscription_out = sub_client.create_subscription(subscription_path_out, topic_path_out)\n",
        "  print(f\"created topic: {subscription_out.name}\")\n",
        "except gexc.AlreadyExists:\n",
        "    print(\"Skipping subscription creation, already exists\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping subscription creation, already exists\n",
            "Skipping subscription creation, already exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z46OxhHChtxI",
        "outputId": "05e54d4b-7c5b-4633-fb6e-6d6c45638890"
      },
      "source": [
        "for subscription in sub_client.list_subscriptions(project_path):\n",
        "    print(subscription.name)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "projects/starthack-workshop/subscriptions/subscription_out\n",
            "projects/starthack-workshop/subscriptions/subscription_in\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH7Y64xrjvKi"
      },
      "source": [
        "# Modeling\n",
        "\n",
        "We need a model to classify tweets as disaster or not. \n",
        "\n",
        "**The purpose of this workshop is not modeling, so we will not spend to much time on it and go for an easy solution. The pre-processing and modeling are purposedly rudimentary.**\n",
        "\n",
        "We build the model as a SKlearn pipeline, it has the advantage to bundle multiple steps into one estimator. We can then have our preprocessing included in the model:\n",
        "\n",
        "1. First we select the column of interest in the DataFrame, in our case the text column. This column does not contain any NA values so ...\n",
        "2. We vectorize the raw text using TFIDF ([TFIDF details](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), [Sklearn TFIDF](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html))\n",
        "3. We apply a Random Forest classifier ([Sklearn RandomForestClassfier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g41_Y6Tw1Yg6"
      },
      "source": [
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "from google.cloud import storage\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "wRiNljO_JLM_",
        "outputId": "d30129ad-040f-467e-926d-4d7972fbfc52"
      },
      "source": [
        "# Read the data from our Google Cloud bucket\n",
        "df_train = pd.read_csv(\"gs://starthack-workshop-data/train.csv\")\n",
        "\n",
        "df_train.dropna().head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>48</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Birmingham</td>\n",
              "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>49</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Est. September 2012 - Bristol</td>\n",
              "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>50</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>AFRICA</td>\n",
              "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>52</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Philadelphia, PA</td>\n",
              "      <td>Crying out for more! Set me ablaze</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>53</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>London, UK</td>\n",
              "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id keyword  ...                                               text target\n",
              "31  48  ablaze  ...  @bbcmtd Wholesale Markets ablaze http://t.co/l...      1\n",
              "32  49  ablaze  ...  We always try to bring the heavy. #metal #RT h...      0\n",
              "33  50  ablaze  ...  #AFRICANBAZE: Breaking news:Nigeria flag set a...      1\n",
              "34  52  ablaze  ...                 Crying out for more! Set me ablaze      0\n",
              "35  53  ablaze  ...  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...      0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppr3pUCkYxzI"
      },
      "source": [
        "# Explain model with schema\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uR-BLx31lqm"
      },
      "source": [
        "def select_text(df: pd.DataFrame) -> pd.Series:\n",
        "    return df[\"text\"]\n",
        "\n",
        "# Model pipeline\n",
        "pipe = Pipeline([\n",
        "    ('selector', FunctionTransformer(select_text)),\n",
        "    ('tfidf', TfidfVectorizer(lowercase=True)),\n",
        "    ('model', RandomForestClassifier(random_state=42))\n",
        "\n",
        "])\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "UxC7kcGf6EgF",
        "outputId": "5a1343aa-8c32-485e-b799-4cd1be12b5b8"
      },
      "source": [
        "# OPTIONAL - cross-validation step to get an idea of model performances\n",
        "\n",
        "cv_results = cross_validate(pipe, df_train, df_train['target'], cv=3, scoring=[\"roc_auc\"], return_train_score=True, n_jobs=-1)\n",
        "cv_results = pd.DataFrame(cv_results)\n",
        "cv_results.agg((\"mean\", \"std\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fit_time</th>\n",
              "      <th>score_time</th>\n",
              "      <th>test_roc_auc</th>\n",
              "      <th>train_roc_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>7.989036</td>\n",
              "      <td>0.267601</td>\n",
              "      <td>0.764883</td>\n",
              "      <td>0.999892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.193525</td>\n",
              "      <td>0.055022</td>\n",
              "      <td>0.037528</td>\n",
              "      <td>0.000082</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      fit_time  score_time  test_roc_auc  train_roc_auc\n",
              "mean  7.989036    0.267601      0.764883       0.999892\n",
              "std   1.193525    0.055022      0.037528       0.000082"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baIg4s8s_fuN"
      },
      "source": [
        "model = pipe.fit(df_train, df_train[\"target\"])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHov_EhOl0dQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "e9a997bb-0100-4d66-9b8f-5edbc269fb3b"
      },
      "source": [
        "# We pickle and save the model to our Google Cloud bucket\n",
        "\n",
        "pickle.dump(model, open(\"model.pkl\", 'wb'))\n",
        "bucket = storage.Client().get_bucket('starthack-workshop-data')\n",
        "blob = bucket.blob('model.pkl')\n",
        "blob.upload_from_filename('model.pkl')\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-b3a57bcc1665>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We pickle and save the model to our Google Cloud bucket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'starthack-workshop-data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-3JqE8dcOWz"
      },
      "source": [
        "# Beam Pipeline Overview\n",
        "\n",
        "We can summarize beam pipelines as follow:\n",
        "1. Read data from any source into a `PCollection` A\n",
        "2. Transform A into another `PCollection` B by applying a `PTranform` (map, filter, ...)\n",
        "3. Transform B intot another `PCollection`\n",
        "4. ... keep chaing tranformers\n",
        "5. Save the final output somewhere\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "![beam-pipeline-doc](https://beam.apache.org/images/design-your-pipeline-linear.svg \"Windowing\")\n",
        "[*Image taken from official documentation*](https://beam.apache.org/documentation/programming-guide/#transforms)\n",
        "\n",
        "<br/>\n",
        "\n",
        "## Initialization\n",
        "\n",
        "We first need an pipeline. It will be the entry point:\n",
        "\n",
        "```python\n",
        "pipeline = beam.Pipeline()\n",
        "```\n",
        "\n",
        "Or to create a pipeline and run it locally directly:\n",
        "\n",
        "```python\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "    ...\n",
        "```\n",
        "\n",
        "## Applying a transform on your pipeline\n",
        "\n",
        "Use the **pipe** operator `|` to apply a transform on a collection. You can pair it with the `>>` to give a name to the transformation:\n",
        "\n",
        "Without name:\n",
        "\n",
        "```python\n",
        "pipeline | transform\n",
        "```\n",
        "\n",
        "With name:\n",
        "\n",
        "```python\n",
        "pipeline | \"step_name\" >> transform\n",
        "```\n",
        "\n",
        "## Read data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbzxmnoywjUF"
      },
      "source": [
        "import apache_beam as beam"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VurOXSSLm27y",
        "outputId": "ffddaa42-b857-4e28-e7b7-650eb6f25996"
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "    (pipeline\n",
        "        | \"read_data\" >> beam.Create([1, 2, 3, 4, 5])\n",
        "        | \"print\" >> beam.Map(lambda x: print(x, end=', '))\n",
        "    )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1, 2, 3, 4, 5, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvzwqOeVmjyD"
      },
      "source": [
        "\n",
        "## Common operators\n",
        "\n",
        "### beam.Map (map a function on a collection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeLzIK1UlXod",
        "outputId": "ae7f7a40-f4c3-483a-f1c0-5ca1bbdae32f"
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "    (pipeline\n",
        "        | \"read_data\" >> beam.Create([1, 2, 3, 4, 5])\n",
        "        | \"add_one\" >> beam.Map(lambda x: x + 1)\n",
        "        | \"print\" >> beam.Map(lambda x: print(x, end=', '))\n",
        "    )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2, 3, 4, 5, 6, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcw2t0J8keRH"
      },
      "source": [
        "### beam.ParDo with beam.DoFn\n",
        "\n",
        "Beam has a generic transform for data processing: `beam.ParDo`. \n",
        "It takes as input a function to apply: `beam.DoFn`. \n",
        "\n",
        "### ParDo\n",
        "1. run on each element in the input `PCollection`\n",
        "2. apply a processing function\n",
        "3. emit **zero**, **one**, or **many outputs** for each input element\n",
        "\n",
        "It's generic and can implement any usual transform: map, flatmap, filter, ...\n",
        "Indeed all these are special cases of `beam.ParDo` processing.\n",
        "\n",
        "\n",
        "#### DoFn\n",
        "`beam.DoFn` represent a processing function applied by the `beam.ParDo`.\n",
        "\n",
        "A beam.DoFn function **should always return an iterable or None**. beam.ParDo will flatten the iterable.\n",
        "\n",
        "\n",
        "For instance if you want to return one element, you need to wrap it into a list or tuple:\n",
        "\n",
        "```\n",
        "1. 1 -> Pardo()\n",
        "2. DoFnMultBy2(1) -> [2]\n",
        "3. ParDo([2]) -> 2\n",
        "```\n",
        "\n",
        "The first dimension of your output is flattened. \n",
        "\n",
        "Another nice feature is **stateful computation**. Since `beam.DoFn` is a class, it is possible to have an **internal state**. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LwAxRlf9c9z"
      },
      "source": [
        "Quick example: add an incremental index, starting from N, to elements\n",
        "\n",
        "```\n",
        "Input: 1, 2, 3, 4, 5\n",
        "\n",
        "Apply IdxFn\n",
        "\n",
        "Output:  (N, 1), (N+1, 2), (N+2, 3), (N+3, 4), (N+4, 5)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnDVvN6lkevQ",
        "outputId": "6e269a40-09e0-4229-dc4d-facec2c68d74"
      },
      "source": [
        "class IdxFn(beam.DoFn):\n",
        "    \"\"\"Custom DoFn function to add an index. It will be applied using beam.ParDo\"\"\"\n",
        "    def __init__(self, init_state: int):\n",
        "        # this is is the first number to use as an index, we will increment it for each element\n",
        "        self.state = init_state\n",
        "\n",
        "    def process(self, elem: Any) -> Iterable[Any]:\n",
        "        # create a tuple with the current state as index and the element\n",
        "        res = (self.state, elem)\n",
        "        # increment the state\n",
        "        self.state += 1\n",
        "        return [res]\n",
        "\n",
        "print(\"Like a Map:\")\n",
        "with beam.Pipeline() as pipeline:\n",
        "    (pipeline\n",
        "        | \"read_data\" >> beam.Create([1, 2, 3, 4, 5])\n",
        "        | \"with_idx\" >> beam.ParDo(IdxFn(init_state=10))\n",
        "        | \"print\" >> beam.Map(lambda x: print(x, end=', '))\n",
        "    )\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Like a Map:\n",
            "(10, 1), (11, 2), (12, 3), (13, 4), (14, 5), "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcLlyyyWlpIy"
      },
      "source": [
        "### beam.Filter (filter a collection) - Optional, not used in this workshop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVgVTUs5ipIz",
        "outputId": "b7ff7f81-d469-4a0a-a45b-cd5741fce473"
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "    (pipeline\n",
        "        | \"read_data\" >> beam.Create([1, 2, 3, 4, 5])\n",
        "        | \"keep_even_only\" >> beam.Filter(lambda x: (x % 2) == 0)\n",
        "        | \"print\" >> beam.Map(lambda x: print(x, end=', '))\n",
        "    )"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2, 4, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFMqrBAglrZ4"
      },
      "source": [
        "### beam.FlatMap (filter a collection) - Optional, not used in this workshop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIzW48URleVn",
        "outputId": "23f28475-5f47-4b1e-eb72-cdfcd9244c9f"
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "    (pipeline\n",
        "        | \"read_data\" >> beam.Create([1, 2, 3, 4, 5])\n",
        "        | \"duplicate_inputs\" >> beam.FlatMap(lambda x: [x, x])\n",
        "        | \"print\" >> beam.Map(lambda x: print(x, end=', '))\n",
        "    )"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1, 1, 2, 2, 3, 3, 4, 4, 5, 5, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWSvhW3_j4xe"
      },
      "source": [
        "### beam.window.WindowInto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-XeoYLPwgDF"
      },
      "source": [
        "#### Without window\n",
        "\n",
        "![without-window](https://i.postimg.cc/N0Nksgrx/without-window.png\n",
        "\"Windowing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htCT4SRawmhf"
      },
      "source": [
        "#### With session window\n",
        "\n",
        "![with-window](https://i.postimg.cc/cLFWzjCV/with-window.png\n",
        "\"Windowing\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLu3MEbaerc5",
        "outputId": "2bf23fbd-b51b-4d83-ed8a-5448e344990d"
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "    input_stream = [(1, 100), (2, 101), (3, 102), (4, 200)]\n",
        "    input_stream = [beam.window.TimestampedValue(elem, timestamp) for elem, timestamp in input_stream]\n",
        "    pc = (pipeline\n",
        "        | beam.Create(input_stream)\n",
        "        | 'window' >> beam.WindowInto(beam.window.FixedWindows(5))\n",
        "        | 'group' >> beam.GroupBy()\n",
        "        | 'pp' >> beam.Map(lambda x: print(f\"Batch: {x}\"))\n",
        "    )"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch: (Key(), [1, 2, 3])\n",
            "Batch: (Key(), [4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-9a_g1VI4U6"
      },
      "source": [
        "# Our Beam Pipeline\n",
        "\n",
        "As a reminder, this is what we want:\n",
        "\n",
        "![beam pipeline](https://i.postimg.cc/xT5TvZnJ/beam-pipeline-focus.png\n",
        "\"Beam Pipeline\")\n",
        "\n",
        "We will focus on two important parts: **Grouping by chunk** of time and **Predicting**.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teGW6YTwJ3jy"
      },
      "source": [
        "# Predict\n",
        "\n",
        "The most custom and critical step here is the **prediction**. Indeed we need to build a custom `beam.DoFn` with an initialization step pulling the pickled model from Google Cloud and loading the model. Then use the model to transform the incoming tweets. \n",
        "\n",
        "Here are the steps for the prediction part:\n",
        "1. we implement a custom `beam.DoFn` class called `ApplyModel`\n",
        "2. in the `__init__` method we pull and load the model\n",
        "3. we implement the `process` method which receives batch of tweets and use the model to label them as *DISASTER* or *NORMAL*\n",
        "\n",
        "\n",
        "![dofn](https://i.postimg.cc/KYpdxdBT/dofn.png\n",
        "\"Windowing\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRb9F-MPG9vp"
      },
      "source": [
        "import re\n",
        "import json"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKHFOAt7KDv4"
      },
      "source": [
        "class ApplyModel(beam.DoFn):\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the function\"\"\"\n",
        "        self._model = None\n",
        "        # We import in the init statement to have the library available even when we run the pipeline \n",
        "        # on remote executors such as DataFlow\n",
        "        from google.cloud import storage\n",
        "        import pandas as pd\n",
        "        import pickle\n",
        "        self._pd = pd\n",
        "        bucket = storage.Client().get_bucket('starthack-workshop-data')\n",
        "        blob = bucket.get_blob('model.pkl')\n",
        "        self._model = pickle.loads(blob.download_as_string())\n",
        "\n",
        "     \n",
        "    def process(self, group: Tuple[Any, Iterable[Any]]) -> Iterable[Any]:\n",
        "    #def process(self, group):\n",
        "        \"\"\"Process every batch of tweets\"\"\"\n",
        "        # extract batch and build DataFrame\n",
        "        _, batch = group\n",
        "        df = self._pd.DataFrame(batch)\n",
        "        # predict and format\n",
        "        y = self._model.predict(df)\n",
        "        df['prediction'] = y\n",
        "        df.loc[df[\"prediction\"] == 0, \"prediction\"] = \"NORMAL\"\n",
        "        df.loc[df[\"prediction\"] == 1, \"prediction\"] = \"DISASTER\"\n",
        "        res = df.to_dict(orient=\"records\")\n",
        "        return res\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "def build_pipeline(pipeline: beam.Pipeline) -> beam.Pipeline:\n",
        "    \"\"\"Takes an empty beam.Pipeline in input and returns the full beam.Pipeline\"\"\"\n",
        "    \n",
        "    # first read incoming Tweets from Pub/Sub\n",
        "    messages = (\n",
        "        pipeline\n",
        "        | beam.io.ReadFromPubSub(subscription=subscription_path_in).\n",
        "        with_output_types(bytes))\n",
        "    \n",
        "    # decode from bytes to text and then to JSON\n",
        "    parse = (messages\n",
        "        | 'decode' >> beam.Map(lambda x: x.decode('utf-8'))\n",
        "        | 'to_json' >> beam.Map(lambda x: json.loads(x))\n",
        "    )\n",
        "    \n",
        "    # group the tweets in time windows, this will be our batches\n",
        "    group = (parse \n",
        "        | 'window' >> beam.WindowInto(beam.window.FixedWindows(4))\n",
        "        | 'group' >> beam.GroupBy()\n",
        "    )\n",
        "\n",
        "    def debug_fn(json_with_prediction: Dict) -> Dict:\n",
        "        \"\"\"Pretty print for model predictions\"\"\"\n",
        "        txt = json_with_prediction[\"text\"]\n",
        "\n",
        "        dots = re.findall('.{1,80}', ' '*len(txt))\n",
        "        txt = re.findall('.{1,80}', txt)\n",
        "\n",
        "        for txt_line, dots_line in zip(txt, dots):\n",
        "            if json_with_prediction[\"prediction\"] == \"NORMAL\":\n",
        "                ff = f\"{txt_line:80s} | {dots_line:80s}\"\n",
        "            else:\n",
        "                ff = f\"{dots_line:80s} | {txt_line:80s}\"\n",
        "            print(ff)\n",
        "\n",
        "        print(f\"{' '*80} | {' '*80}\")\n",
        "\n",
        "        return json_with_prediction\n",
        "    \n",
        "    # make predictions and converts them to JSON then to bytes\n",
        "    predict = (\n",
        "        group\n",
        "        | 'predict' >> beam.ParDo(ApplyModel())\n",
        "        | 'debug' >> beam.Map(debug_fn)\n",
        "        | 'to_bytes' >> beam.Map(lambda x: json.dumps(x).encode(\"utf-8\"))\n",
        "    )\n",
        "\n",
        "    # write the results to Pub/Sub\n",
        "    write = (\n",
        "        predict\n",
        "        | \"Write to PubSub\" >> beam.io.WriteToPubSub(topic=topic_path_out, with_attributes=False)\n",
        "    )\n",
        "\n",
        "\n",
        "    return pipeline"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVVKI7fWUbKS"
      },
      "source": [
        "# How to run a pipeline\n",
        "\n",
        "## Locally\n",
        "\n",
        "We can run a Beam pipeline locally using the `DirectRunner`, it's useful to for debugging and tesing.\n",
        "\n",
        "## Remotely\n",
        "In production, we want to run the pipeline into a remote executor for maximum performance and potentially scaling capabilities. \n",
        "In this workshop we will use the Google Cloud DataFlow executor. \n",
        "\n",
        "## How we proceed\n",
        "\n",
        "We will first run it locally to make sure that everything runs smoothly.\n",
        "\n",
        "Then we will run it on Google Cloud DataFlow runner to simulate a production application."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB1PJsUlXcxG"
      },
      "source": [
        "from apache_beam.runners import DataflowRunner\n",
        "from apache_beam.options.pipeline_options import PipelineOptions, SetupOptions, StandardOptions, GoogleCloudOptions"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mq6-AGz1KSpB",
        "outputId": "5e9c88f1-c2ef-41c7-fd61-0322274ed81b"
      },
      "source": [
        "print()\n",
        "print(f\"{'NORMAL':80s} | {'DISASTER':80s}\")\n",
        "print(f\"{'':80s} | {'':80s}\")\n",
        "\n",
        "options = PipelineOptions()\n",
        "options.view_as(StandardOptions).streaming = True\n",
        "\n",
        "with beam.Pipeline(options=options) as pipeline:\n",
        "    build_pipeline(pipeline)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "NORMAL                                                                           | DISASTER                                                                        \n",
            "                                                                                 |                                                                                 \n",
            "Apocalypse lighting. #Spokane #wildfires                                         |                                                                                 \n",
            "                                                                                 |                                                                                 \n",
            "                                                                                 | Just happened a terrible car crash                                              \n",
            "                                                                                 |                                                                                 \n",
            "                                                                                 | Typhoon Soudelor kills 28 in China and Taiwan                                   \n",
            "                                                                                 |                                                                                 \n",
            "Heard about #earthquake is different cities, stay safe everyone.                 |                                                                                 \n",
            "                                                                                 |                                                                                 \n",
            "                                                                                 | there is a forest fire at spot pond, geese are fleeing across the street, I cann\n",
            "                                                                                 | ot save them all                                                                \n",
            "                                                                                 |                                                                                 \n",
            "We're shaking...It's an earthquake                                               |                                                                                 \n",
            "                                                                                 |                                                                                 \n",
            "What if?!                                                                        |                                                                                 \n",
            "                                                                                 |                                                                                 \n",
            "No don't tell me that!                                                           |                                                                                 \n",
            "                                                                                 |                                                                                 \n",
            "                                                                                 | Birmingham Wholesale Market is ablaze BBC News - Fire breaks out at Birmingham's\n",
            "                                                                                 |  Wholesale Market http://t.co/irWqCEZWEU                                        \n",
            "                                                                                 |                                                                                 \n",
            "                                                                                 | #PreviouslyOnDoyinTv: Toke MakinwaÛªs marriage crisis sets Nigerian Twitter abl\n",
            "                                                                                 | aze... http://t.co/CMghxBa2XI                                                   \n",
            "                                                                                 |                                                                                 \n",
            "They'd probably still show more life than Arsenal did yesterday, eh? EH?         |                                                                                 \n",
            "                                                                                 |                                                                                 \n",
            "What a nice hat?                                                                 |                                                                                 \n",
            "                                                                                 |                                                                                 \n",
            "Fuck off!                                                                        |                                                                                 \n",
            "                                                                                 |                                                                                 \n",
            "No I don't like cold!                                                            |                                                                                 \n",
            "                                                                                 |                                                                                 \n",
            "Awesome!                                                                         |                                                                                 \n",
            "                                                                                 |                                                                                 \n",
            "PSA: IÛªm splitting my personalities.                                           |                                                                                 \n",
            "?? techies follow @ablaze_co                                                     |                                                                                 \n",
            "                                                                                 |                                                                                 \n",
            "Check these out: http://t.co/rOI2NSmEJJ http://t.co/3Tj8ZjiN21 http://t.co/YDUiX |                                                                                 \n",
            "EfIpE http://t.co/LxTjc87KLS #nsfw                                               |                                                                                 \n",
            "                                                                                 |                                                                                 \n",
            "beware world ablaze sierra leone &amp; guap.                                     |                                                                                 \n",
            "                                                                                 |                                                                                 \n",
            "                                                                                 | Burning Man Ablaze! by Turban Diva http://t.co/hodWosAmWS via @Etsy             \n",
            "                                                                                 |                                                                                 \n",
            "Not a diss song. People will take 1 thing and run with it. Smh it's an eye opene |                                                                                 \n",
            "r though. He is about 2 set the game ablaze @CyhiThePrynce                       |                                                                                 \n",
            "                                                                                 |                                                                                 \n",
            "                                                                                 | Rape victim dies as she sets herself ablaze: A 16-year-old girl died of burn inj\n",
            "                                                                                 | uries as she set herself ablazeÛ_ http://t.co/UK8hNrbOob                       \n",
            "                                                                                 |                                                                                 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-dfb1a4b15324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mbuild_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    579\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/apache_beam/runners/direct/direct_runner.py\u001b[0m in \u001b[0;36mwait_until_finish\u001b[0;34m(self, duration)\u001b[0m\n\u001b[1;32m    588\u001b[0m             'DirectRunner does not support duration argument.')\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawait_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m       \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/apache_beam/runners/direct/executor.py\u001b[0m in \u001b[0;36mawait_completion\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mawait_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawait_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/apache_beam/runners/direct/executor.py\u001b[0m in \u001b[0;36mawait_completion\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mawait_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisible_updates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/apache_beam/runners/direct/executor.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    557\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m           \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "635AmXoqpN73"
      },
      "source": [
        "# Set up Apache Beam pipeline options.\n",
        "options = PipelineOptions(streaming=True)\n",
        "\n",
        "# Set the project to the default project in your current Google Cloud\n",
        "# environment.\n",
        "_, options.view_as(GoogleCloudOptions).project = google.auth.default()\n",
        "\n",
        "# Set the Google Cloud region to run Dataflow.\n",
        "options.view_as(GoogleCloudOptions).region = 'europe-west6'\n",
        "\n",
        "# Choose a Cloud Storage location.\n",
        "dataflow_gcs_location = 'gs://starthack-workshop-data/dataflow'\n",
        "\n",
        "# Set the staging location. This location is used to stage the\n",
        "# Dataflow pipeline and SDK binary.\n",
        "options.view_as(GoogleCloudOptions).staging_location = '%s/staging' % dataflow_gcs_location\n",
        "\n",
        "# Set the temporary location. This location is used to store temporary files\n",
        "# or intermediate results before outputting to the sink.\n",
        "options.view_as(GoogleCloudOptions).temp_location = '%s/temp' % dataflow_gcs_location\n",
        "\n",
        "# Set the SDK location. This is used by Dataflow to locate the\n",
        "# SDK needed to run the pipeline.\n",
        "#options.view_as(SetupOptions).sdk_location = (\n",
        "#    '/root/apache-beam-custom/packages/beam/sdks/python/dist/apache-beam-%s0.tar.gz' %\n",
        "#    beam.version.__version__)\n",
        "\n",
        "pipe = build_pipeline(beam.Pipeline())\n",
        "runner = DataflowRunner()\n",
        "runner.run_pipeline(pipe, options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-89vzdGi7wQ"
      },
      "source": [
        "# Subscribe to the output stream"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-AvK5M55dGt"
      },
      "source": [
        "import pprint\n",
        "\n",
        "def callback(message: str):\n",
        "    \"\"\"Callback to deal with the received message\"\"\"\n",
        "    x = json.loads(message.data.decode(\"utf-8\"))\n",
        "    print()\n",
        "    print(x)\n",
        "    print()\n",
        "    message.ack()\n",
        "\n",
        "\n",
        "\n",
        "# Subscribe to the topics\n",
        "streaming_pull_future = sub_client.subscribe(subscription_path_out, callback=callback)\n",
        "print(f\"Listening for messages on {subscription_path_out} ...\\n\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        # When `timeout` is not set, result() will block indefinitely,\n",
        "        # unless an exception is encountered first.\n",
        "        streaming_pull_future.result()\n",
        "    except TimeoutError:\n",
        "        streaming_pull_future.cancel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meOJi9BeK28i"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "\n",
        "## Machine Learning on Streams \n",
        "Processing streams with machine learning has many applications:\n",
        "1. IoT sensors\n",
        "2. Server logs ...\n",
        "\n",
        "\n",
        "## Apache Beam\n",
        "Apache Beam facilitate stream manipulation. It has numerous advantages:\n",
        "1. Multiple compatible backends\n",
        "2. Available in multiple programming languages\n",
        "3. Built-in support for batch and **streaming** processing\n",
        "\n",
        "\n",
        "## Serverless\n",
        "We used multiple managed services (Storage, Pub/Sub, DataFlow). It simple to setup and it scales smoothly.\n",
        "Just be careful with the bill, features like auto-scaling can surprise you.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZzuxZgFNnNd"
      },
      "source": [
        "# Thank You For Attending !\n",
        "\n",
        "## We hope this workshop will be helpful !"
      ]
    }
  ]
}